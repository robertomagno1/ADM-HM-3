{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Import Necessary Libraries and Define Preprocessing Functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Preprocessing the Text\n",
    "\n",
    "This preprocessing pipeline standardizes and cleans textual data for efficient search queries. The steps are as follows:\n",
    "---\n",
    "\n",
    "### Step 1: Tokenization\n",
    "- Splits text into individual words (tokens).\n",
    "- Removes punctuation and converts text to lowercase.\n",
    "\n",
    "**Example Input:**  \n",
    "*\"Michelin-starred restaurant serves exquisite dishes in Paris!\"*  \n",
    "**Processed Tokens:**  \n",
    "`[\"michelinstarred\", \"restaurant\", \"serves\", \"exquisite\", \"dishes\", \"paris\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Stopword Removal\n",
    "- Removes common words like \"and\", \"is\", \"the\" that do not contribute meaning.  \n",
    "**Before:**  \n",
    "`[\"michelinstarred\", \"restaurant\", \"serves\", \"exquisite\", \"dishes\", \"paris\"]`  \n",
    "**After:**  \n",
    "`[\"michelinstarred\", \"restaurant\", \"exquisite\", \"dishes\", \"paris\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Stemming\n",
    "- Reduces words to their base forms.  \n",
    "**Examples:**  \n",
    "`\"serving\" → \"serv\"`  \n",
    "`\"dishes\" → \"dish\"`\n",
    "\n",
    "**Final Tokens:**  \n",
    "`[\"michelinstar\", \"restaurant\", \"exquisit\", \"dish\", \"paris\"]`\n",
    "\n",
    "---\n",
    "\n",
    "## Processed Columns\n",
    "Preprocessing is applied to the following:\n",
    "- `restaurant_name`\n",
    "- `description`\n",
    "- `city`\n",
    "- `country`\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Preprocessing in Action\n",
    "\n",
    "**Raw Description:**  \n",
    "*\"Michelin-starred restaurant offering exquisite French cuisine in Paris.\"*  \n",
    "**Processed Description:**  \n",
    "`\"michelinstar restaur offer exquisit french cuisin pari\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Initialize NLP tools for preprocessing\n",
    "stop_words = set(stopwords.words('english'))  # Common stopwords\n",
    "stemmer = PorterStemmer()  # Stemmer for reducing words to their base forms\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text:\n",
    "    - Remove punctuation\n",
    "    - Convert to lowercase\n",
    "    - Remove stopwords\n",
    "    - Apply stemming for normalization\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    tokens = text.lower().split()\n",
    "    return [stemmer.stem(word) for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Create Vocabulary and Inverted Index (Task 2.1.1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Building the Vocabulary and Inverted Index\n",
    "\n",
    "The vocabulary and inverted index are fundamental components for efficient text-based search. Here’s a breakdown of their creation:\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Build Vocabulary\n",
    "- Maps each unique word in the dataset to a unique integer (Term ID).\n",
    "- **Input:** Preprocessed descriptions.\n",
    "- **Output:** A dictionary where keys are words and values are Term IDs.\n",
    "\n",
    "**Example Vocabulary:**  \n",
    "| Word          | Term ID |\n",
    "|---------------|---------|\n",
    "| \"dish\"        | 0       |\n",
    "| \"restaurant\"  | 1       |\n",
    "| \"paris\"       | 2       |\n",
    "\n",
    "### Step 2: Build Inverted Index\n",
    "\n",
    "- Maps each Term ID to a list of Document IDs where the term appears.\n",
    "- Input: Preprocessed descriptions and vocabulary.\n",
    "- Output: A dictionary where keys are Term IDs and values are lists of Document IDs.\n",
    "\n",
    "\n",
    "| Term ID       | Document ID |\n",
    "|---------------|---------    |\n",
    "| 0        | \t[1, 3, 5]           |\n",
    "| 1  | [0, 2, 4]           |\n",
    "| 2       |   [2]           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary and inverted index created and saved to files.\n"
     ]
    }
   ],
   "source": [
    "def build_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Create a vocabulary mapping each unique word in the dataset to a unique integer (term ID).\n",
    "    \"\"\"\n",
    "    return {word: idx for idx, word in enumerate(sorted(set(word for doc in documents for word in doc)))}\n",
    "\n",
    "def build_inverted_index(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    Create an inverted index that maps each term ID to the list of document IDs where the term appears.\n",
    "    \"\"\"\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for word in doc:\n",
    "            if word in vocabulary:  # Only process words in the vocabulary\n",
    "                term_id = vocabulary[word]\n",
    "                if doc_id not in inverted_index[term_id]:  # Avoid duplicate entries\n",
    "                    inverted_index[term_id].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\39339\\Desktop\\ADM\\HW3\\michelin_restaurants.csv\")\n",
    "\n",
    "# Preprocess restaurant descriptions\n",
    "descriptions = [preprocess_text(desc) for desc in df['description']]\n",
    "\n",
    "# Create vocabulary and inverted index\n",
    "vocabulary = build_vocabulary(descriptions)  # Map words to unique IDs\n",
    "inverted_index = build_inverted_index(descriptions, vocabulary)  # Map term IDs to document IDs\n",
    "\n",
    "# Save vocabulary and inverted index for future use\n",
    "vocabulary_path = r\"C:\\Users\\39339\\Desktop\\ADM\\HW3\\vocabulary.csv\"\n",
    "inverted_index_path = r\"C:\\Users\\39339\\Desktop\\ADM\\HW3\\inverted_index.json\"\n",
    "\n",
    "# Save vocabulary as CSV\n",
    "pd.DataFrame(list(vocabulary.items()), columns=[\"Word\", \"Term ID\"]).to_csv(vocabulary_path, index=False)\n",
    "\n",
    "# Save inverted index as JSON\n",
    "with open(inverted_index_path, 'w') as f:\n",
    "    json.dump(inverted_index, f)\n",
    "\n",
    "print(f\"Vocabulary and inverted index created and saved to files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Execute Conjunctive Query (Task 2.1.2)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Executing a Conjunctive Query\n",
    "\n",
    "A conjunctive query retrieves documents (restaurants) where **all query words** are present in their descriptions. Below is a concise breakdown of how it works:\n",
    "\n",
    "---\n",
    "\n",
    "### **Function: `conjunctive_query`**\n",
    "This function performs the following steps:\n",
    "1. **Preprocess the Query:**\n",
    "   - Tokenizes, cleans, and stems the query text.\n",
    "   - Converts query words into their corresponding Term IDs using the vocabulary.\n",
    "\n",
    "2. **Find Matching Documents:**\n",
    "   - Intersects the lists of Document IDs for all Term IDs in the query.\n",
    "   - Ensures that only documents containing all query terms are returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Conjunctive Query Execution\n",
    "\n",
    "### **Input Query**\n",
    "*\"Michelin-starred fine dining in Paris\"*\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "\n",
    "1. **Preprocess the Query**  \n",
    "   - **Tokens:**  \n",
    "     `[\"michelinstar\", \"fine\", \"dine\", \"pari\"]`\n",
    "\n",
    "2. **Convert Tokens to Term IDs**  \n",
    "   - Using the vocabulary:  \n",
    "     `[\"michelinstar\" → 12, \"fine\" → 34, \"dine\" → 56, \"pari\" → 78]`  \n",
    "   - **Term IDs:**  \n",
    "     `[12, 34, 56, 78]`\n",
    "\n",
    "3. **Retrieve Matching Documents**  \n",
    "   - Using the inverted index, find documents containing all terms:  \n",
    "     **Matching Document IDs:** `[0, 5, 9]`\n",
    "\n",
    "4. **Output Results**  \n",
    "   - Display restaurant names, addresses, descriptions, and websites for matching documents.\n",
    "\n",
    "---\n",
    "\n",
    "### **Results Table**\n",
    "\n",
    "| Restaurant Name       | Address          | Description                                      | Website               |\n",
    "|-----------------------|------------------|------------------------------------------------|-----------------------|\n",
    "| Le Jules Verne        | Eiffel Tower     | A Michelin-starred restaurant in Paris.         | www.lejulesverne.com |\n",
    "| L'Astrance            | Rue Beethoven    | Fine dining experience with exquisite cuisine.  | www.lastrance.com    |\n",
    "| Epicure               | Rue Saint-Honoré | Luxurious dining at the heart of Paris.         | www.epicure.com      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Number of Matching Restaurants**  \n",
    "`3`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurantName</th>\n",
       "      <th>address</th>\n",
       "      <th>description</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Osteria Numero 2</td>\n",
       "      <td>via Ghisiolo 2/a</td>\n",
       "      <td>A beautiful farmhouse not far from the town, w...</td>\n",
       "      <td>https://www.osterianumero2.it/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     restaurantName           address  \\\n",
       "0  Osteria Numero 2  via Ghisiolo 2/a   \n",
       "\n",
       "                                         description  \\\n",
       "0  A beautiful farmhouse not far from the town, w...   \n",
       "\n",
       "                          website  \n",
       "0  https://www.osterianumero2.it/  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching restaurants: 1\n"
     ]
    }
   ],
   "source": [
    "def conjunctive_query(query, vocabulary, inverted_index):\n",
    "    \"\"\"\n",
    "    Execute a conjunctive query:\n",
    "    - Find restaurants where all query words are present in their description.\n",
    "    \"\"\"\n",
    "    query_tokens = preprocess_text(query)  # Preprocess the query terms\n",
    "    term_ids = [vocabulary[word] for word in query_tokens if word in vocabulary]  # Map query words to term IDs\n",
    "\n",
    "    if not term_ids:  # If no query words are in the vocabulary, return empty\n",
    "        return []\n",
    "\n",
    "    # Find documents containing all the terms (intersection of lists)\n",
    "    matching_docs = set(inverted_index[term_ids[0]])  # Start with the first term's document list\n",
    "    for term_id in term_ids[1:]:\n",
    "        matching_docs &= set(inverted_index.get(term_id, []))  # Intersect with subsequent term's document lists\n",
    "\n",
    "    return list(matching_docs)  # Return the matching document IDs\n",
    "\n",
    "\n",
    "# Input query from user\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Execute conjunctive query\n",
    "matching_docs = conjunctive_query(query, vocabulary, inverted_index)\n",
    "\n",
    "# Create a results table\n",
    "table = []\n",
    "for idx in matching_docs:\n",
    "    row = df.iloc[idx]\n",
    "    table.append({\n",
    "        \"restaurantName\": row['restaurantName'],  # Restaurant name\n",
    "        \"address\": row['address'],               # Address\n",
    "        \"description\": row['description'],       # Description\n",
    "        \"website\": row['website']                # Website URL\n",
    "    })\n",
    "\n",
    "# Display the results table in the desired format\n",
    "display(pd.DataFrame(table))\n",
    "print(f\"Number of matching restaurants: {len(matching_docs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Build Ranked Search Engine with TF-IDF (Task 2.2.1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Conjunctive Query & Ranking Score**\n",
    "\n",
    "For the second Search Engine, given a query, we want to get the *top-k* (in this case, we chose $k=5$) documents related to the query and a **similarity** measure. In particular, we chose to perform the following procedure:\n",
    "\n",
    "1. We built a dictionary containing all the words found in the `description` column of each document (or reused a previously built vocabulary). Using this vocabulary, we built the **TfIdf inverted index** of the words. The TfIdf inverted index is a dictionary of the form:\n",
    "\n",
    "    ```\n",
    "    {\n",
    "    term_id_1:[(document_1, tfIdf_{term,document1}), (document_2, tfIdf_{term,document2}), (document_4, tfIdf_{term,document4})],\n",
    "    term_id_2:[(document_1, tfIdf_{term,document1}), (document_3, tfIdf_{term,document3}), (document_5, tfIdf_{term,document5}), (document_6, tfIdf_{term,document6})],\n",
    "    ...}\n",
    "    ```\n",
    "\n",
    "    where `document_i` is the *id* of a document that contains a specific word, the `term_id_i` is the *id* of a specific word in the vocabulary, and the `TfIdf` is the Term Frequency-Inverse Document Frequency value of the `term_id_i` within `document_i`. The Term Frequency-Inverse Document Frequency is given by:\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\text{TfIdf}(t,d) = \\text{Tf}(t,d) \\times \\text{Idf}(t) =  \\text{Tf}(t,d) \\times \\log \\left(\\frac{N}{1+df}\\right),\n",
    "    \\end{equation}\n",
    "\n",
    "    where $\\text{Tf}(t,d)$ is the (normalized) frequency of times the term $t$ appears in document $d$, and $\\text{Idf}(t)$ is the **inverse document frequency** of term $t$, where $df$ is the number of documents that include term $t$ and $N$ is the total number of documents. The purpose of the Idf is to give higher weight to terms that are rare across the entire corpus of text and lower weight to terms that are common.\n",
    "\n",
    "    The TfIdf index gives a mapping from every word in the vocabulary to all the documents that contain it and a measurement of **how important** that word is within each document relative to its importance across all documents. \n",
    "\n",
    "2. Once we built (and saved) the vocabulary and TfIdf inverted index, we took a **query** as input. As a first step, we preprocess the query and then search for all the documents that contain **all** of the words/tokens in the query. To perform this search, we take advantage of the TfIdf inverted index since we only have to take the intersection of the lists of the terms contained in the query (the lists of the first elements of the tuple).\n",
    "\n",
    "3. Once we got all the relevant documents, we decided to sort them by their **Cosine Similarity** with respect to the query. The Cosine Similarity is a vector similarity measure that measures how similar two vectors $\\vec{A}$ and $\\vec{B}$ are by taking the angle $\\theta$ between them. It is given by:\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\text{sim}\\left(\\vec{A}, \\vec{B}\\right) = \\cos(\\theta) = \\frac{\\vec{A}\\cdot\\vec{B}}{|\\vec{A}||\\vec{B}|}.\n",
    "    \\tag{2}\n",
    "    \\end{equation}\n",
    "\n",
    "    In the context of NLP, we can represent documents as vectors where each vector value is the tfIdf representation of a term within the document. Therefore, we can obtain the similarity between documents by taking the **cosine similarity** of their tfIdf representations. In this case, we obtained the cosine similarity between the query and all the obtained documents. \n",
    "    \n",
    "    Once we had a similarity value for all the relevant documents, we **sorted** them in descending order with respect to the similarity with the query. To maintain the top-$k$ documents efficiently, we used a **max-heap** data structure.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2.2.1 Inverted Index**\n",
    "\n",
    "As in the first Search Engine we built, the TfIdf inverted index is a fundamental tool for the Search Engine. This is why the index is computed *before* making any query and saved into memory. This allows it to be loaded when needed instead of being recalculated every time. To achieve this, the index is incorporated as a class **attribute**, so it is loaded into memory each time the `TopKSearchEngine` class is initialized.\n",
    "\n",
    "As an exercise, we can observe the first 5 elements of the TfIdf inverted index value for the \"data\" term as follows:\n",
    "\n",
    "```python\n",
    "term_id = vocabulary[\"data\"]\n",
    "print(tfidf_inverted_index[term_id][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores computed and updated inverted index saved.\n"
     ]
    }
   ],
   "source": [
    "def compute_tf(document):\n",
    "    \"\"\"\n",
    "    Compute term frequency (TF):\n",
    "    - Calculate how often each word appears in the document relative to its length.\n",
    "    \"\"\"\n",
    "    tf = defaultdict(int)\n",
    "    for word in document:\n",
    "        tf[word] += 1\n",
    "    return {word: count / len(document) for word, count in tf.items()}  # Normalize by document length\n",
    "\n",
    "def compute_idf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    Compute inverse document frequency (IDF):\n",
    "    - Measures the importance of a word across the entire dataset.\n",
    "    \"\"\"\n",
    "    num_docs = len(documents)  # Total number of documents\n",
    "    doc_freq = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        unique_words = set(doc)  # Consider only unique words in each document to avoid repetitions for a word\n",
    "        for word in unique_words:\n",
    "            if word in vocabulary:\n",
    "                doc_freq[word] += 1 #increasing the count if the word is in a document\n",
    "    return {word: math.log((num_docs + 1) / (doc_freq[word] + 1)) + 1 for word in vocabulary}\n",
    "\n",
    "def compute_tfidf(document, idf):\n",
    "    \"\"\"\n",
    "    Compute the tf-idf scores combining term frequency (TF) \n",
    "    and inverse document frequency (IDF) for a given document.\n",
    "    \"\"\"\n",
    "    tf = compute_tf(document)\n",
    "    return {word: tf[word] * idf[word] for word in document if word in idf}\n",
    "\n",
    "# Calculate IDF for all words in the vocabulary\n",
    "idf = compute_idf(descriptions, vocabulary)\n",
    "\n",
    "# Calculate TF-IDF scores for all documents\n",
    "tfidf = [compute_tfidf(doc, idf) for doc in descriptions]\n",
    "\n",
    "# Build updated inverted index with TF-IDF scores\n",
    "tfidf_inverted_index = defaultdict(list)\n",
    "for doc_id, doc_tfidf in enumerate(tfidf):\n",
    "    for word, score in doc_tfidf.items():\n",
    "        term_id = vocabulary[word]\n",
    "        tfidf_inverted_index[term_id].append((doc_id, score))  # Store document ID and TF-IDF score\n",
    "\n",
    "# Save updated inverted index\n",
    "tfidf_inverted_index_path = r\"C:\\Users\\39339\\Desktop\\ADM\\HW3\\inverted_index.json\"\n",
    "with open(tfidf_inverted_index_path, 'w') as f:\n",
    "    json.dump(tfidf_inverted_index, f)\n",
    "\n",
    "print(\"TF-IDF scores computed and updated inverted index saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Ranked Query (Task 2.2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Conjunctive query & Ranking score**\n",
    "\n",
    "For the second Search Engine, given a query, we want to get the *top-k* (in this case, we chose $k=5$) documents related to the query and a **similarity** measure. In particular we chose to perform the following procedure:\n",
    "\n",
    "1. We built a dictionary containing all the words found in the `description` column of each course (in reality, use the one we created before). Using this vocabulary we built the **TfIdf inverted index** of the words. The TfIdf inverted index is a dictionary of the form:\n",
    "    ```\n",
    "    {\n",
    "    term_id_1:[(document_1, tfIdf_{term,document1}), (document_2, tfIdf_{term,document2}), (document_4, tfIdf_{term,document4})],\n",
    "    term_id_2:[(document_1, tfIdf_{term,document1}), (document_3, tfIdf_{term,document3}), (document_5, tfIdf_{term,document5}), (document_6, tfIdf_{term,document6})],\n",
    "    ...}\n",
    "    ```\n",
    "    where `document_i` is the *id* of a document that contains a specific word, the `term_id_i` is the *id* of a specific word in the vocabulary, and the `TfIdf` is the Term Frequency-Inverse Document Frequency value of the `term_id_i` within `document_i`. The Term Frequency-Inverse Document Frequency is given by:\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\text{TfIdf}(t,d) = \\text{Tf}(t,d) \\times \\text{Idf}(t) =  \\text{Tf}(t,d) \\times \\log \\left(\\frac{N}{1+df}\\right),\n",
    "    \\end{equation}\n",
    "\n",
    "    where $\\text{Tf}(t,d)$ is the (normalized) frequency of times the term $t$ appears in document $d$ and $\\text{Idf}(t)$ is the **inverse document frequency** of term $t$, where $df$ is the number of documents that include term $t$ and $N$ is the total number of documents. The purpose of the Idf is to give higher weight to terms that are rare across the entire corpus of a text and lower weight to terms that are common.\n",
    "\n",
    "    The TfIdf index gives a mapping from every word in the vocabulary to all the documents that contain it and a measurement **how important** is that word within each document relative to its importance across all documents. \n",
    "\n",
    "2. Once we built (and saved) the vocabulary and TfIdf inverted index, we take a **query** as an input. As a first step we preprocess the query and then search for all the documents that contain **all** of the words/tokens in the query. To perform this search we takes advantage of the TfIdf inverted index since we only have to take the intersection of the lists of the terms contained in the query (the lists of the first elements of the tuple).\n",
    "\n",
    "3. Once we got all the relevant documents, we decided to sort them by their **Cosine Similarity** with respect to the query. The Cosine Similarity is a vector similarity measure that measures how similar are two vectors $\\vec{A}$ and $\\vec{B}$ by taking the angle $\\theta$ between them. It is given by:\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\text{sim}\\left(\\vec{A}, \\vec{B}\\right) = cos(\\theta) = \\frac{\\vec{A}\\cdot\\vec{B}}{|\\vec{A}||\\vec{B}|}.\n",
    "    \\tag{2}\n",
    "    \\end{equation}\n",
    "\n",
    "    In the context of NLP, we can represent documents as vectors where each vector value is the tfIdf representation of a term within this document. Therefore, we can obtain the similarity between documents by taking the **cosine similarity** of their tfIdf representation. In this case, we obtained the cosine similarity between the query and all the obtained documents. \n",
    "    \n",
    "    Once we had a similarity value for all the relevant documents, we **sorted** them in descending value with respect to the similarity with the query. In order to mantain the top-$k$ documents in an efficient way, we used a **max-heap** data structure.\n",
    "\n",
    "#### **2.2.1 Inverted index**\n",
    "\n",
    "As in the first Search Engine we built, the TfIdf inverted index is a fundamental tool for building of our Search Engine, this is why they are obtained *before* making any query to the Search Engine and saved into memory. In this way they are loaded into memory when necessary instead of being calculated each time. To do this we incorporated them as a class **attribute** so it was loaded into memory each time the `TopKSearchEngine` class is initialized.\n",
    "\n",
    "As an exercise, we can observe the first 5 elements of the TfIdf inverted index value for the \"data\" term as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Description</th>\n",
       "      <th>Website</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saur</td>\n",
       "      <td>via Filippo Turati 8</td>\n",
       "      <td>In a tiny rural village, this contemporary, al...</td>\n",
       "      <td>https://ristorantesaur.it</td>\n",
       "      <td>0.3061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razzo</td>\n",
       "      <td>via Andrea Doria 17/f</td>\n",
       "      <td>A quiet restaurant with a relaxed, young and m...</td>\n",
       "      <td>https://vadoarazzo.it/</td>\n",
       "      <td>0.2719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La Botte</td>\n",
       "      <td>via Giuseppe Garibaldi 8</td>\n",
       "      <td>A modern and welcoming contemporary bistro sit...</td>\n",
       "      <td>http://www.trattorialabottestresa.it</td>\n",
       "      <td>0.2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Piccolo Lord</td>\n",
       "      <td>corso San Maurizio 69 bis/g</td>\n",
       "      <td>Professional service in a welcoming, modern re...</td>\n",
       "      <td>https://www.ristorantepiccololord.it/</td>\n",
       "      <td>0.2507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Valle</td>\n",
       "      <td>via Umberto I 25</td>\n",
       "      <td>A well - run restaurant in a quiet area just o...</td>\n",
       "      <td>https://www.ristorantelavalle.it/</td>\n",
       "      <td>0.2408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Restaurant Name                      Address  \\\n",
       "0            Saur         via Filippo Turati 8   \n",
       "1           Razzo        via Andrea Doria 17/f   \n",
       "2        La Botte     via Giuseppe Garibaldi 8   \n",
       "3    Piccolo Lord  corso San Maurizio 69 bis/g   \n",
       "4        La Valle             via Umberto I 25   \n",
       "\n",
       "                                         Description  \\\n",
       "0  In a tiny rural village, this contemporary, al...   \n",
       "1  A quiet restaurant with a relaxed, young and m...   \n",
       "2  A modern and welcoming contemporary bistro sit...   \n",
       "3  Professional service in a welcoming, modern re...   \n",
       "4  A well - run restaurant in a quiet area just o...   \n",
       "\n",
       "                                 Website Similarity Score  \n",
       "0              https://ristorantesaur.it           0.3061  \n",
       "1                 https://vadoarazzo.it/           0.2719  \n",
       "2   http://www.trattorialabottestresa.it           0.2632  \n",
       "3  https://www.ristorantepiccololord.it/           0.2507  \n",
       "4      https://www.ristorantelavalle.it/           0.2408  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Measures similarity between the tf-idf scores of the given query and each document.\n",
    "    \"\"\"\n",
    "    common_words = set(vec1.keys()) & set(vec2.keys())  # Find common words\n",
    "    numerator = sum(vec1[word] * vec2[word] for word in common_words)  # Dot product\n",
    "    norm_vec1 = math.sqrt(sum(val ** 2 for val in vec1.values()))  # Magnitude of vec1\n",
    "    norm_vec2 = math.sqrt(sum(val ** 2 for val in vec2.values()))  # Magnitude of vec2\n",
    "    return numerator / (norm_vec1 * norm_vec2) if norm_vec1 and norm_vec2 else 0  # In case one of the vectors are 0(common words = 0), it returns 0\n",
    "\n",
    "def ranked_query(query, k=5):\n",
    "    \"\"\"\n",
    "    Find and rank restaurants based on cosine similarity between the query and each document.\n",
    "    \"\"\"\n",
    "    query_tokens = preprocess_text(query)  # Preprocess the query terms\n",
    "    query_tfidf = compute_tfidf(query_tokens, idf)  # Compute TF-IDF for the query\n",
    "\n",
    "    scores = []\n",
    "    for doc_id, doc_tfidf in enumerate(tfidf):\n",
    "        similarity = cosine_similarity(query_tfidf, doc_tfidf)  # Compute similarity\n",
    "        scores.append((doc_id, similarity))  # Store document ID and similarity score\n",
    "\n",
    "    # Sort by similarity score and return top-k results\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:k]\n",
    "\n",
    "# Input query from user\n",
    "query = input(\"Enter your query: \")\n",
    "#k = int(input(\"Enter the number of top results to display: \"))\n",
    "\n",
    "# Execute ranked query\n",
    "top_k_results = ranked_query(query, k=5)\n",
    "\n",
    "# Display results in a table\n",
    "results_table = [\n",
    "    {\n",
    "        \"Restaurant Name\": df.iloc[doc_id][\"restaurantName\"],\n",
    "        \"Address\": df.iloc[doc_id][\"address\"],\n",
    "        \"Description\": df.iloc[doc_id][\"description\"],\n",
    "        \"Website\": df.iloc[doc_id][\"website\"],\n",
    "        \"Similarity Score\": f\"{score:.4f}\"\n",
    "    }\n",
    "    for doc_id, score in top_k_results\n",
    "]\n",
    "\n",
    "display(pd.DataFrame(results_table))  # Print the results table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3. Define a New Score!\n",
    "Now, we will define a custom ranking metric to prioritize restaurants based on user queries.\n",
    "\n",
    "Steps:\n",
    "* User Query: The user provides a text query. We’ll retrieve relevant documents using the search engine built in Step 2.1.\n",
    "* New Ranking Metric: After retrieving relevant documents, we’ll rank them using a new custom score. Instead of limiting the scoring to only the description field, we can include other attributes like priceRange, facilitiesServices, and cuisineType.\n",
    "* You will use a heap data structure (e.g., Python’s heapq library) to maintain the top-k restaurants.\n",
    "\n",
    "#### New Scoring Function:  \n",
    "Define a scoring function that takes into account various attributes:\n",
    "* Description Match: Give weight based on the query similarity to the description (using TF-IDF scores).\n",
    "* Cuisine Match: Increase the score for matching cuisine types.\n",
    "Facilities and Services: Give more points for matching facilities/services (e.g., “Terrace,” “Air conditioning”).\n",
    "* Price Range: Higher scores could be given to more affordable options based on the user’s choice.\n",
    "\n",
    "### Output:\n",
    "The output should include:\n",
    "* restaurantName\n",
    "* address\n",
    "* description\n",
    "* website\n",
    "* The new similarity score based on the custom metric.  \n",
    "\n",
    "Are the results you obtain better than with the previous scoring function? Explain and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Define a New Scoring System!**\n",
    "\n",
    "Can we do better? In the last implementation of our Search Engine, we sorted results by the **cosine similarity** of the TF-IDF representation of their `description` field with respect to the query. However, this approach does not account for the following considerations:\n",
    "\n",
    "1. A restaurant can be **more relevant** to a query if it matches the query across multiple fields. For example:\n",
    "    - If we search for the query *\"pizza\"* and it appears not only in the `description` but also in the `cuisineType` or `facilitiesServices`, these fields should contribute to the ranking score. Matching across multiple fields provides a broader understanding of relevance.\n",
    "\n",
    "2. Users often prioritize restaurants that meet **additional preferences** such as:\n",
    "    - **Facilities:** For example, a user searching for \"wheelchair access\" should find restaurants where this facility is explicitly listed.\n",
    "    - **Price Compatibility:** Users may prefer restaurants that fit within their desired price range.\n",
    "    - **Cuisine Type:** A user searching for \"Italian\" should find restaurants where this cuisine is explicitly provided.\n",
    "\n",
    "To address these points, we propose a **weighted** scoring system that considers the following fields: `description`, `cuisineType`, `facilitiesServices`, and `priceRange`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.1 Custom Weighted Scoring Formula**\n",
    "\n",
    "Our new scoring metric combines the cosine similarity of the query with multiple fields of the dataset (`description`, `cuisineType`, `facilitiesServices`) and considers the price compatibility. The formula is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Score} = w_{\\text{desc}} \\cdot \\text{CS}_{\\text{desc}} + w_{\\text{cuis}} \\cdot \\text{CS}_{\\text{cuis}} + w_{\\text{facil}} \\cdot \\text{CS}_{\\text{facil}} + w_{\\text{price}} \\cdot \\text{Sim}_{\\text{price}}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Key Components**\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{similarity}(\\vec{d}, \\vec{q}) = \\frac{w_{\\text{context}}}{3} \\cdot \\left[\\text{cs}_{\\text{description}}(\\vec{d}, \\vec{q}) + \\text{cs}_{\\text{cuis}}(\\vec{d}, \\vec{q}) + \\text{cs}_{\\text{facil}}(\\vec{d}, \\vec{q})\\right] + \\frac{w_{\\text{price}}}{1} \\cdot \\text{Sim}_{\\text{price}}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- **Cosine similarities:**\n",
    "  \\begin{equation}\n",
    "  \\text{CS}_{\\text{desc}}, \\text{CS}_{\\text{cuis}}, \\text{CS}_{\\text{facil}}\n",
    "  \\tag{3}\n",
    "  \\end{equation}\n",
    "  These represent the cosine similarities for `description`, `cuisineType`, and `facilitiesServices`, respectively.\n",
    "\n",
    "- **Price compatibility:**\n",
    "  \\begin{equation}\n",
    "  \\text{Sim}_{\\text{price}}\n",
    "  \\tag{4}\n",
    "  \\end{equation}\n",
    "  Quantifies how compatible the restaurant's price range is with the user's maximum price preference.\n",
    "\n",
    "- **Weights:**\n",
    "  \\begin{equation}\n",
    "  w_{\\text{desc}}, w_{\\text{cuis}}, w_{\\text{facil}}, w_{\\text{price}}\n",
    "  \\tag{5}\n",
    "  \\end{equation}\n",
    "  These are the weights assigned to each factor, controlling their relative importance in the overall score.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Key Components**\n",
    "\n",
    "#### **Cosine Similarity for Text Fields**\n",
    "\n",
    "Cosine similarity is used to compute the alignment between the TF-IDF representation of the query and each field. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{CS}(\\vec{A}, \\vec{B}) = \\frac{\\vec{A} \\cdot \\vec{B}}{\\|\\vec{A}\\| \\cdot \\|\\vec{B}\\|}\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "This ensures that fields with higher overlap with the query receive higher scores.\n",
    "\n",
    "#### **Price Compatibility**\n",
    "\n",
    "For the price similarity component:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Sim}_{\\text{price}}\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "We assign a higher score to restaurants whose price range does not exceed the user's maximum budget.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.4 Implementation**\n",
    "\n",
    "We implemented this scoring system in Python using the following key functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Address</th>\n",
       "      <th>Description</th>\n",
       "      <th>Website</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Osteria Numero 2</td>\n",
       "      <td>via Ghisiolo 2/a</td>\n",
       "      <td>A beautiful farmhouse not far from the town, w...</td>\n",
       "      <td>https://www.osterianumero2.it/</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remo Villa Cariolato</td>\n",
       "      <td>strada di Bertesina 313</td>\n",
       "      <td>Long recommended by the Michelin Guide, the hi...</td>\n",
       "      <td>https://www.removillacariolato.it</td>\n",
       "      <td>0.6205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La Passion</td>\n",
       "      <td>via San Nicolò 5/b</td>\n",
       "      <td>A small, cosy, completely wood - panelled stub...</td>\n",
       "      <td>https://www.lapassion.it/it/</td>\n",
       "      <td>0.5754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vert Osteria Contemporanea</td>\n",
       "      <td>Località Bogonza</td>\n",
       "      <td>Housed in a rustic building on the green slope...</td>\n",
       "      <td>https://vertosteria.it/</td>\n",
       "      <td>0.5426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aubergine</td>\n",
       "      <td>via Ghislandi 5</td>\n",
       "      <td>Situated in a town famous for its thermal bath...</td>\n",
       "      <td>https://www.ristoranteaubergine.it/</td>\n",
       "      <td>0.5234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Restaurant Name                  Address  \\\n",
       "0            Osteria Numero 2         via Ghisiolo 2/a   \n",
       "1        Remo Villa Cariolato  strada di Bertesina 313   \n",
       "2                  La Passion       via San Nicolò 5/b   \n",
       "3  Vert Osteria Contemporanea         Località Bogonza   \n",
       "4                   Aubergine          via Ghislandi 5   \n",
       "\n",
       "                                         Description  \\\n",
       "0  A beautiful farmhouse not far from the town, w...   \n",
       "1  Long recommended by the Michelin Guide, the hi...   \n",
       "2  A small, cosy, completely wood - panelled stub...   \n",
       "3  Housed in a rustic building on the green slope...   \n",
       "4  Situated in a town famous for its thermal bath...   \n",
       "\n",
       "                               Website   Score  \n",
       "0       https://www.osterianumero2.it/  1.0000  \n",
       "1    https://www.removillacariolato.it  0.6205  \n",
       "2         https://www.lapassion.it/it/  0.5754  \n",
       "3              https://vertosteria.it/  0.5426  \n",
       "4  https://www.ristoranteaubergine.it/  0.5234  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from heapq import nlargest\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialize NLP tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a given text by removing special characters, extra spaces, \n",
    "    and stopwords. Applying stemming and then lemmatization. \n",
    "    The function splits the text into words, so the output is normalized for further processing.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    tokens = text.lower().split()\n",
    "    return [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens if word not in stop_words]\n",
    "\n",
    "def build_vocabulary(documents):\n",
    "    \"\"\"\n",
    "    Create a vocabulary from a list of tokenized documents.\n",
    "    Each unique word is mapped to a unique index to avoid that the same word has \n",
    "    different indexes.\n",
    "    \"\"\"\n",
    "    return {word: idx for idx, word in enumerate(sorted(set(word for doc in documents for word in doc)))}\n",
    "\n",
    "def optimized_get_idf(documents, vocabulary):\n",
    "    \"\"\"\n",
    "    Calculate the IDF scores for all words in the vocabulary\n",
    "    \"\"\"\n",
    "    doc_freq = defaultdict(int)\n",
    "    for doc in documents:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            if word in vocabulary:\n",
    "                doc_freq[word] += 1\n",
    "    num_docs = len(documents)\n",
    "    return {word: math.log((num_docs + 1) / (doc_freq[word] + 1)) + 1 for word in vocabulary}\n",
    "\n",
    "def compute_tfidf(document, idf):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF scores for a given document.\n",
    "    \"\"\"\n",
    "    tf = defaultdict(int)\n",
    "    for word in document:\n",
    "        tf[word] += 1\n",
    "    return {word: (tf[word] / len(document)) * idf[word] for word in document if word in idf}\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    common_words = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum(vec1[word] * vec2[word] for word in common_words)\n",
    "    norm_vec1 = math.sqrt(sum(val ** 2 for val in vec1.values()))\n",
    "    norm_vec2 = math.sqrt(sum(val ** 2 for val in vec2.values()))\n",
    "    return numerator / (norm_vec1 * norm_vec2) if norm_vec1 and norm_vec2 else 0\n",
    "\n",
    "def compute_custom_score(tfidf_query_desc, tfidf_desc, tfidf_query_cuis, tfidf_cuis, tfidf_query_facil, tfidf_facil, max_price, doc_price, weights):\n",
    "    \"\"\"\n",
    "    Compute a custom score for ranking, based on a weighted combination of:\n",
    "    - Cosine similarity for text descriptions\n",
    "    - Matching of cuisine preferences\n",
    "    - Matching of desired facilities\n",
    "    - Price similarity between the query and the document\n",
    "    The weights determine the relative importance of each component in the final score.\n",
    "    \"\"\"\n",
    "    w_desc, w_cuis, w_facil, w_price = weights\n",
    "\n",
    "    # Cosine similarity for description\n",
    "    sim_desc = cosine_similarity(tfidf_query_desc, tfidf_desc) * w_desc\n",
    "\n",
    "    # Matching cuisines\n",
    "    matching_cuis = cosine_similarity(tfidf_query_cuis, tfidf_cuis)* w_cuis\n",
    "\n",
    "    # Matching facilities\n",
    "    matching_facil =  cosine_similarity(tfidf_query_facil, tfidf_facil) * w_facil\n",
    "\n",
    "    # Price similarity\n",
    "    #sim_price = (1 / (1 + abs(max_price - doc_price))) * w_price\n",
    "    sim_price = w_price if doc_price <= max_price else 0\n",
    "\n",
    "    # Total score\n",
    "    return sim_desc + matching_cuis + matching_facil + sim_price\n",
    "\n",
    "def get_top_k(query, cuis, facil, max_price, descriptions, prices, idf_desc, tfidf_desc, idf_cuis, tfidf_cuis, idf_facil, tfidf_facil, k=5, weights=(0.4, 0.2, 0.2, 0.2)):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k ranked restaurants based on the given custom scoring metric.\n",
    "    This function processes user input, calculates query vectors, and ranks \n",
    "    the restaurants based on their similarity to the query and other criteria.\n",
    "    \"\"\"\n",
    "    # Preprocess the queries\n",
    "    query_tokens = preprocess_text(query)\n",
    "    cuis_tokens = preprocess_text(cuis)\n",
    "    facil_tokens = preprocess_text(facil)\n",
    "\n",
    "    # Compute query TF-IDF\n",
    "    tfidf_query_desc = compute_tfidf(query_tokens, idf_desc)\n",
    "    tfidf_query_cuis = compute_tfidf(cuis_tokens, idf_cuis)\n",
    "    tfidf_query_facil = compute_tfidf(facil_tokens, idf_facil)\n",
    "\n",
    "    scores = []\n",
    "    for idx in range(len(descriptions)):\n",
    "        # Compute custom score\n",
    "        score = compute_custom_score(\n",
    "            tfidf_query_desc,\n",
    "            tfidf_desc[idx],\n",
    "            tfidf_query_cuis,\n",
    "            tfidf_cuis[idx],\n",
    "            tfidf_query_facil,\n",
    "            tfidf_facil[idx],\n",
    "            max_price,\n",
    "            prices[idx],\n",
    "            weights\n",
    "        )\n",
    "        scores.append((idx, score))\n",
    "\n",
    "    # Get top-k results using a heap\n",
    "    return nlargest(k, scores, key=lambda x: x[1])\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(r\"C:\\Users\\39339\\Desktop\\ADM\\HW3\\michelin_restaurants.csv\")\n",
    "    descriptions = [preprocess_text(desc) for desc in df['description']]\n",
    "    cuisines = [preprocess_text(cuisine) for cuisine in df['cuisineType']]\n",
    "    facilities = [preprocess_text(facility) for facility in df['facilitiesServices']]\n",
    "    prices = [len(price) for price in df['priceRange']]  # Convert € symbols to numeric scale\n",
    "\n",
    "    # Build vocabularies and IDF\n",
    "    vocabulary_desc = build_vocabulary(descriptions)\n",
    "    idf_desc = optimized_get_idf(descriptions, vocabulary_desc)\n",
    "\n",
    "    vocabulary_cuis = build_vocabulary(cuisines)\n",
    "    idf_cuis = optimized_get_idf(cuisines, vocabulary_cuis)\n",
    "\n",
    "    vocabulary_facil = build_vocabulary(facilities)\n",
    "    idf_facil = optimized_get_idf(facilities, vocabulary_facil)\n",
    "\n",
    "    # Compute TF-IDF for documents\n",
    "    tfidf_desc = [compute_tfidf(doc, idf_desc) for doc in descriptions]\n",
    "    tfidf_cuis = [compute_tfidf(cuis, idf_cuis) for cuis in cuisines]\n",
    "    tfidf_facil = [compute_tfidf(facil, idf_facil) for facil in facilities]\n",
    "\n",
    "\n",
    "    # User inputs\n",
    "    query = input(\"Enter your query for the description: \")\n",
    "    cuis = input(\"Enter the cuisine types: \")\n",
    "    facil = input(\"Enter the facilities: \")\n",
    "    max_price = len(input(\"Enter the maximum price (€, €€, etc.): \").strip())  # Convert € symbols to numeric\n",
    "\n",
    "    # Get top-k results\n",
    "    top_k_results = get_top_k(query, cuis, facil, max_price, descriptions, prices, idf_desc, tfidf_desc, idf_cuis, tfidf_cuis, idf_facil, tfidf_facil, k=5)\n",
    "\n",
    "    # Display results\n",
    "    results_table = [\n",
    "        {\n",
    "            \"Restaurant Name\": df.iloc[idx][\"restaurantName\"],\n",
    "            \"Address\": df.iloc[idx][\"address\"],\n",
    "            \"Description\": df.iloc[idx][\"description\"],\n",
    "            \"Website\": df.iloc[idx][\"website\"],\n",
    "            \"Score\": f\"{score:.4f}\"\n",
    "        }\n",
    "        for idx, score in top_k_results\n",
    "    ]\n",
    "\n",
    "    display(pd.DataFrame(results_table))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
